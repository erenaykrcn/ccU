{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "975a4b35-6817-408f-8526-a9318a74bacb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time step 0/8, layer 0/3 applied\n",
      "Time step 0/8, layer 1/3 applied\n",
      "Time step 0/8, layer 2/3 applied\n",
      "Time step 1/8, layer 0/3 applied\n",
      "Time step 1/8, layer 1/3 applied\n",
      "Time step 1/8, layer 2/3 applied\n",
      "Time step 2/8, layer 0/3 applied\n",
      "Time step 2/8, layer 1/3 applied\n",
      "Time step 2/8, layer 2/3 applied\n",
      "Time step 3/8, layer 0/3 applied\n",
      "Time step 3/8, layer 1/3 applied\n",
      "Time step 3/8, layer 2/3 applied\n",
      "Time step 4/8, layer 0/3 applied\n",
      "Time step 4/8, layer 1/3 applied\n",
      "Time step 4/8, layer 2/3 applied\n",
      "Time step 5/8, layer 0/3 applied\n",
      "Time step 5/8, layer 1/3 applied\n",
      "Time step 5/8, layer 2/3 applied\n",
      "Time step 6/8, layer 0/3 applied\n",
      "Time step 6/8, layer 1/3 applied\n",
      "Time step 6/8, layer 2/3 applied\n",
      "Time step 7/8, layer 0/3 applied\n",
      "Time step 7/8, layer 1/3 applied\n",
      "Time step 7/8, layer 2/3 applied\n",
      "Norm. Trotter\n",
      "norm finished Trotter\n",
      "Step 1 took 0.03 seconds\n",
      "Step 1 took 0.05 seconds\n",
      "Step 2 took 0.03 seconds\n",
      "Step 2 took 0.04 seconds\n",
      "Step 3 took 0.03 seconds\n",
      "Step 3 took 0.04 seconds\n",
      "Step 6 took 0.03 seconds\n",
      "Step 6 took 0.38 seconds\n",
      "Step 7 took 0.03 seconds\n",
      "Step 7 took 0.38 seconds\n",
      "Step 8 took 0.03 seconds\n",
      "Step 8 took 0.38 seconds\n",
      "Step 11 took 0.03 seconds\n",
      "Step 11 took 0.38 seconds\n",
      "Step 12 took 0.03 seconds\n",
      "Step 12 took 0.38 seconds\n",
      "Step 13 took 0.03 seconds\n",
      "Step 13 took 0.38 seconds\n",
      "Normalize ccU\n",
      "Normalization of ccU finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8951612282465274"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../src/brickwall_sparse\")\n",
    "from utils_sparse import applyG_block_state, get_perms\n",
    "\n",
    "import numpy as np\n",
    "import quimb.tensor as qtn\n",
    "import quimb\n",
    "from quimb.tensor.tensor_2d_tebd import TEBD2D, LocalHam2D\n",
    "import scipy\n",
    "import h5py\n",
    "import qib\n",
    "import rqcopt as oc\n",
    "from scipy.sparse.linalg import expm_multiply\n",
    "\n",
    "import time\n",
    "import tracemalloc\n",
    "tracemalloc.start()\n",
    "\n",
    "\n",
    "Vlist = []\n",
    "with h5py.File(f\"./results/tfim2d_ccU_SPARSE_103_Lx4Ly4_t0.25_layers15_rS1_niter15_3hloc.hdf5\", \"r\") as f:\n",
    "    Vlist =  f[\"Vlist\"][:]\n",
    "control_layers = [0, 4, 5, 9, 10, 14]\n",
    "perms_qc = [[0, 1], [0, 2], [1, 2], [0, 2], [0, 1], [1, 2], [0, 2], [0, 1], [1, 2]]\n",
    "Xlists_opt = {}\n",
    "for i in control_layers:\n",
    "    with h5py.File(f\"./results/tfim2d_ccU_SPARSE_103_Lx4Ly4_t0.25_layers15_niter20_rS5_DECOMPOSE_n9_layer{i}.hdf5\", \"r\") as file:\n",
    "        Xlists_opt[i] = file[f\"Xlist_{i}\"][:]\n",
    "\n",
    "Lx, Ly = (6, 6)\n",
    "L = Lx*Ly\n",
    "latt = qib.lattice.IntegerLattice((Lx, Ly), pbc=True)\n",
    "field = qib.field.Field(qib.field.ParticleType.QUBIT, latt)\n",
    "J, h, g = (1, 0, 3)\n",
    "#hamil = qib.IsingHamiltonian(field, J, h, g).as_matrix()\n",
    "\n",
    "#perms_v, perms_h = get_perms(Lx, Ly)\n",
    "perms_v, perms_h = (\n",
    "    [[0 , 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35],\n",
    "    [1, 2, 3, 4, 5, 0, 7, 8, 9, 10, 11, 6, 13, 14, 15, 16, 17, 12, 19, 20, 21, 22, 23, 18, 25, 26, 27, 28, 29, 24, 31, 32, 33, 34, 35, 30]],\n",
    "    [[0, 6, 12, 18, 24, 30, 1, 7, 13, 19, 25, 31, 2, 8, 14, 20, 26, 32, 3, 9, 15, 21, 27, 33, 4, 10, 16, 22, 28, 34, 5, 11, 17, 23, 29, 35], \n",
    "    [6, 12, 18, 24, 30, 0, 7, 13, 19, 25, 31, 1, 8, 14, 20, 26, 32, 2, 9, 15, 21, 27, 33, 3, 10, 16, 22, 28, 34, 4, 11, 17, 23, 29, 35, 5]]\n",
    ")\n",
    "perms_extended = [[perms_v[0]]] + [perms_v]*3 + [[perms_v[0]], [perms_h[0]]] +\\\n",
    "                    [perms_h]*3 + [[perms_h[0]], [perms_v[0]]] + [perms_v]*3 + [[perms_v[0]]]\n",
    "perms_ext_reduced = [perms_v]*3  + [perms_h]*3 + [perms_v]*3\n",
    "map_ = {i: (i//Ly, i%Lx) for i in range(L)}\n",
    "\n",
    "peps = qtn.PEPS.rand(Lx, Ly, bond_dim=1, phys_dim=2, cyclic=True)\n",
    "peps /= peps.norm()\n",
    "#sv = peps.to_dense()[:, 0]\n",
    "#sv = expm_multiply(-1j * 0.25 * hamil, sv)\n",
    "peps_trotter = trotter(peps.copy(), 0.25, L, Lx, Ly, J, g, perms_v, perms_h, dt=0.25/8, max_bond_dim=3, lower_max_bond_dim=3, treshold=10)\n",
    "#f = quimb.fidelity(peps_trotter.to_dense()[:, 0], sv)\n",
    "#print(\"Trotter Fidelity:\", f)  # Should be ≈1\n",
    "\n",
    "\n",
    "peps_ccU = ccU(peps.copy(), Vlist, perms_extended, control_layers, dagger=False, max_bond_dim=3, lower_max_bond_dim=3, treshold=10)\n",
    "#f = quimb.fidelity(peps_ccU.to_dense()[:, 0], sv)\n",
    "#print(\"ccU Fidelity after identity:\", f)  # Should be ≈1\n",
    "\n",
    "np.linalg.norm(peps_ccU.overlap(peps_trotter))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2925a975-7ea8-4d51-b6fe-8fc364f28446",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('peps_ccU_MOCK.h5', 'w') as f:\n",
    "    for site, t in enumerate(peps_ccU.tensors):\n",
    "        dset_name = f\"site_{site}\"\n",
    "        dset = f.create_dataset(dset_name, data=t.data)\n",
    "        inds_ascii = [ind.encode('ascii', 'ignore') for ind in t.inds]\n",
    "        dset.attrs['inds'] = inds_ascii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eebe74a8-5c4b-41fc-b6a4-b131372ce452",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9746/1182409636.py:4: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  data = np.array(f[key][:], dtype=np.float64)  # Force NumPy + valid dtype\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "autoray couldn't find function 'squeeze' for backend 'quimb'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/envs/qc/lib/python3.10/site-packages/autoray/autoray.py:548\u001b[0m, in \u001b[0;36mget_lib_fn\u001b[0;34m(backend, fn)\u001b[0m\n\u001b[1;32m    547\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 548\u001b[0m     lib_fn \u001b[38;5;241m=\u001b[39m \u001b[43m_FUNCS\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "\u001b[0;31mKeyError\u001b[0m: ('quimb', 'squeeze')",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/envs/qc/lib/python3.10/site-packages/autoray/autoray.py:515\u001b[0m, in \u001b[0;36mimport_lib_fn\u001b[0;34m(backend, fn)\u001b[0m\n\u001b[1;32m    514\u001b[0m     \u001b[38;5;66;03m# store the function!\u001b[39;00m\n\u001b[0;32m--> 515\u001b[0m     lib_fn \u001b[38;5;241m=\u001b[39m _FUNCS[backend, fn] \u001b[38;5;241m=\u001b[39m wrapper(\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlib\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn_name\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    517\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[1;32m    518\u001b[0m     \u001b[38;5;66;03m# check if there is a backup function (e.g. for older library version)\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'quimb' has no attribute 'squeeze'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 13\u001b[0m\n\u001b[1;32m      6\u001b[0m         tensors\u001b[38;5;241m.\u001b[39mappend(qtn\u001b[38;5;241m.\u001b[39mTensor(data\u001b[38;5;241m=\u001b[39mdata, inds\u001b[38;5;241m=\u001b[39minds))\n\u001b[1;32m      8\u001b[0m arrays \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mtuple\u001b[39m(tensors[i \u001b[38;5;241m*\u001b[39m Ly \u001b[38;5;241m+\u001b[39m j] \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(Ly))\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(Lx)\n\u001b[1;32m     11\u001b[0m )\n\u001b[0;32m---> 13\u001b[0m peps_loaded \u001b[38;5;241m=\u001b[39m \u001b[43mqtn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPEPS\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/qc/lib/python3.10/site-packages/quimb/tensor/tensor_2d.py:4917\u001b[0m, in \u001b[0;36mPEPS.__init__\u001b[0;34m(self, arrays, shape, tags, site_ind_id, site_tag_id, x_tag_id, y_tag_id, **tn_opts)\u001b[0m\n\u001b[1;32m   4915\u001b[0m \u001b[38;5;66;03m# allow convention of missing bonds to be singlet dimensions\u001b[39;00m\n\u001b[1;32m   4916\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ar\u001b[38;5;241m.\u001b[39mndim(array) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(array_order):\n\u001b[0;32m-> 4917\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[43mar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msqueeze\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marray\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4919\u001b[0m transpose_order \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[1;32m   4920\u001b[0m     array_order\u001b[38;5;241m.\u001b[39mfind(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murdlp\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m array_order\n\u001b[1;32m   4921\u001b[0m )\n\u001b[1;32m   4922\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transpose_order \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(array_order))):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/qc/lib/python3.10/site-packages/autoray/autoray.py:80\u001b[0m, in \u001b[0;36mdo\u001b[0;34m(fn, like, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Do function named ``fn`` on ``(*args, **kwargs)``, peforming single\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;124;03mdispatch to retrieve ``fn`` based on whichever library defines the class of\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;124;03mthe ``args[0]``, or the ``like`` keyword argument if specified.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;124;03m    <tf.Tensor: id=91, shape=(3, 3), dtype=float32>\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     79\u001b[0m backend \u001b[38;5;241m=\u001b[39m _choose_backend(fn, args, kwargs, like\u001b[38;5;241m=\u001b[39mlike)\n\u001b[0;32m---> 80\u001b[0m func \u001b[38;5;241m=\u001b[39m \u001b[43mget_lib_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/qc/lib/python3.10/site-packages/autoray/autoray.py:550\u001b[0m, in \u001b[0;36mget_lib_fn\u001b[0;34m(backend, fn)\u001b[0m\n\u001b[1;32m    548\u001b[0m     lib_fn \u001b[38;5;241m=\u001b[39m _FUNCS[backend, fn]\n\u001b[1;32m    549\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m--> 550\u001b[0m     lib_fn \u001b[38;5;241m=\u001b[39m \u001b[43mimport_lib_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m lib_fn\n",
      "File \u001b[0;32m/opt/anaconda3/envs/qc/lib/python3.10/site-packages/autoray/autoray.py:523\u001b[0m, in \u001b[0;36mimport_lib_fn\u001b[0;34m(backend, fn)\u001b[0m\n\u001b[1;32m    520\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m backend_alt \u001b[38;5;129;01min\u001b[39;00m _MODULE_ALIASES:\n\u001b[1;32m    521\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m import_lib_fn(backend_alt, fn)\n\u001b[0;32m--> 523\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m    524\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mautoray couldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find function \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    525\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbackend \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbackend\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[alt]\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    526\u001b[0m     )\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m lib_fn\n",
      "\u001b[0;31mImportError\u001b[0m: autoray couldn't find function 'squeeze' for backend 'quimb'."
     ]
    }
   ],
   "source": [
    "tensors = []\n",
    "with h5py.File('peps_ccU_MOCK.h5', 'r') as f:\n",
    "    for key in sorted(f.keys(), key=lambda x: int(x.split('_')[1])):\n",
    "        data = np.array(f[key][:], dtype=np.float64)  # Force NumPy + valid dtype\n",
    "        inds = list(f[key].attrs['inds'])\n",
    "        tensors.append(qtn.Tensor(data=data, inds=inds))\n",
    "\n",
    "arrays = tuple(\n",
    "    tuple(tensors[i * Ly + j] for j in range(Ly))\n",
    "    for i in range(Lx)\n",
    ")\n",
    "\n",
    "peps_loaded = qtn.PEPS(arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1595ad69-83db-4a43-9d1b-3e4f76130bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rqcopt as oc\n",
    "X = np.array([[0, 1], [1, 0]])\n",
    "Z = np.array([[1, 0], [0, -1]])\n",
    "I2 = np.eye(2)\n",
    "\n",
    "def trotter(peps, t, L, Lx, Ly, J, g, perms_v, perms_h, dag=False, max_bond_dim=5, \n",
    "            dt=0.1, trotter_order=2, treshold=10, lower_max_bond_dim=4):\n",
    "    nsteps = np.abs(int(np.ceil(t/dt)))\n",
    "    t = t/nsteps\n",
    "    indices = oc.SplittingMethod.suzuki(2, int(np.log(trotter_order)/np.log(2))).indices\n",
    "    coeffs = oc.SplittingMethod.suzuki(2, int(np.log(trotter_order)/np.log(2))).coeffs\n",
    "    \n",
    "    hloc1 = g*(np.kron(X, I2)+np.kron(I2, X))/4\n",
    "    hloc2 = J*np.kron(Z, Z)\n",
    "    hlocs = (hloc1, hloc2)\n",
    "    Vlist_start = []\n",
    "    for i, c in zip(indices, coeffs):\n",
    "        Vlist_start.append(-1j*c*t*hlocs[i])\n",
    "\n",
    "    for n in range(nsteps):\n",
    "        for layer, V in enumerate(Vlist_start):\n",
    "            i = n*len(Vlist_start)+layer\n",
    "            for perm in perms_h:\n",
    "                ordering = {(map_[perm[2*j]], map_[perm[2*j+1]]): V for j in range(L//2)}\n",
    "                start = time.time()\n",
    "                t = TEBD2D(peps, ham=LocalHam2D(Lx, Ly, ordering, cyclic=True),\n",
    "                    tau=-1, D=max_bond_dim if i<treshold else lower_max_bond_dim, chi=1)\n",
    "                t.sweep(tau=-1)\n",
    "                peps = t.state\n",
    "                \n",
    "            for perm in perms_v:\n",
    "                ordering = {(map_[perm[2*j]], map_[perm[2*j+1]]): V for j in range(L//2)}\n",
    "                start = time.time()\n",
    "                t = TEBD2D(peps, ham=LocalHam2D(Lx, Ly, ordering, cyclic=True),\n",
    "                    tau=-1, D=max_bond_dim if i<treshold else lower_max_bond_dim, chi=1)\n",
    "                t.sweep(tau=-1)\n",
    "                peps = t.state\n",
    "            #with open(f\"trotter_PEPS_log{Lx}{Ly}.txt\", \"a\") as file:\n",
    "            #    file.write(f\"Time step {n}/{nsteps}, layer {layer}/{len(Vlist_start)} applied \\n\")\n",
    "            print(f\"Time step {n}/{nsteps}, layer {layer}/{len(Vlist_start)} applied\")\n",
    "    print(f\"Norm. Trotter\")\n",
    "    peps /= peps.norm()\n",
    "    print(f\"norm finished Trotter\")\n",
    "    return peps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3546982-58c7-43a5-ba02-4f4b0ee80083",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ccU(peps, Vlist, perms_extended, control_layers, dagger=False, max_bond_dim=10, lower_max_bond_dim=4, treshold=10):\n",
    "    for i, V in enumerate(Vlist):\n",
    "        if dagger or i not in control_layers:\n",
    "            perms = perms_extended[i]\n",
    "            for perm in perms:\n",
    "                ordering = {(map_[perm[2*j]], map_[perm[2*j+1]]): scipy.linalg.logm(V) for j in range(L//2)}\n",
    "                start = time.time()\n",
    "                t = TEBD2D(peps, ham=LocalHam2D(Lx, Ly, ordering, cyclic=True),\n",
    "                    tau=-1, D=max_bond_dim if i<treshold else lower_max_bond_dim, chi=1)\n",
    "                t.sweep(tau=-1)\n",
    "                peps = t.state\n",
    "                #peps /= peps.norm()\n",
    "                print(f\"Step {i} took {time.time() - start:.2f} seconds\")\n",
    "                #print(\"Peak memory:\", tracemalloc.get_traced_memory())\n",
    "    print(f\"Normalize ccU\")\n",
    "    peps /= peps.norm()\n",
    "    print(f\"Normalization of ccU finished\")\n",
    "    return peps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a579c5b1-714e-47fb-8b0d-ad42a31d4ed3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ee8731-2099-4289-812d-8847dda92af4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10cda94-3589-4500-9332-ffef8d50c402",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.10 (qc)",
   "language": "python",
   "name": "qc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
